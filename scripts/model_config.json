{
  "model_name": "llama-3.2-3b-instruct-q4_k_m",
  "model_type": "gguf",
  "model_format": "GGUF",
  "quantization": "Q4_K_M",
  "description": "Llama 3.2 3B Instruct Model (Quantized)",
  "version": "1.0.0",
  "author": "Meta AI",
  "license": "Meta AI License",
  "loading_config": {
    "n_ctx": 8192,
    "n_batch": 512,
    "n_threads": null,
    "n_gpu_layers": 1,
    "verbose": false,
    "use_mmap": true,
    "use_mlock": false,
    "f16_kv": true
  },
  "generation_config": {
    "max_tokens": 32000,
    "temperature": 0.7,
    "top_p": 0.9,
    "frequency_penalty": 0.0,
    "presence_penalty": 0.0,
    "repetition_penalty": 1.1,
    "stop_sequences": []
  },
  "model_info": {
    "parameters": "3.2B",
    "context_length": 8192,
    "vocab_size": 32000,
    "architecture": "LlamaForCausalLM",
    "quantization_method": "Q4_K_M",
    "file_size_gb": 1.9
  },
  "optimization_settings": {
    "platform_optimized": true,
    "memory_efficient": true,
    "gpu_accelerated": true,
    "recommended_device": "mps",
    "fallback_device": "cpu"
  },
  "compatibility": {
    "llama_cpp": true,
    "transformers": false,
    "torch": false,
    "supported_formats": ["gguf"]
  }
} 